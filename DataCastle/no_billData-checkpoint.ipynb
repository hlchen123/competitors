{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#数据准备\" data-toc-modified-id=\"数据准备-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>数据准备</a></div><div class=\"lev1 toc-item\"><a href=\"#自定义函数\" data-toc-modified-id=\"自定义函数-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>自定义函数</a></div><div class=\"lev1 toc-item\"><a href=\"#原始数据读取\" data-toc-modified-id=\"原始数据读取-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>原始数据读取</a></div><div class=\"lev1 toc-item\"><a href=\"#各表处理\" data-toc-modified-id=\"各表处理-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>各表处理</a></div><div class=\"lev2 toc-item\"><a href=\"#user-与overdue表\" data-toc-modified-id=\"user-与overdue表-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>user 与overdue表</a></div><div class=\"lev2 toc-item\"><a href=\"#browse\" data-toc-modified-id=\"browse-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>browse</a></div><div class=\"lev3 toc-item\"><a href=\"#browse日期范围分布\" data-toc-modified-id=\"browse日期范围分布-421\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>browse日期范围分布</a></div><div class=\"lev2 toc-item\"><a href=\"#bank\" data-toc-modified-id=\"bank-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>bank</a></div><div class=\"lev2 toc-item\"><a href=\"#bill\" data-toc-modified-id=\"bill-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>bill</a></div><div class=\"lev3 toc-item\"><a href=\"#bill时间与loan时间\" data-toc-modified-id=\"bill时间与loan时间-441\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>bill时间与loan时间</a></div><div class=\"lev1 toc-item\"><a href=\"#各表合并\" data-toc-modified-id=\"各表合并-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>各表合并</a></div><div class=\"lev2 toc-item\"><a href=\"#prepare-data\" data-toc-modified-id=\"prepare-data-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>prepare data</a></div><div class=\"lev2 toc-item\"><a href=\"#统计查看逾期的数据表的bill，browse以及bank表信息\" data-toc-modified-id=\"统计查看逾期的数据表的bill，browse以及bank表信息-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>统计查看逾期的数据表的bill，browse以及bank表信息</a></div><div class=\"lev2 toc-item\"><a href=\"#strategy--start\" data-toc-modified-id=\"strategy--start-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>strategy -start</a></div><div class=\"lev2 toc-item\"><a href=\"#预测数据集整理\" data-toc-modified-id=\"预测数据集整理-54\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>预测数据集整理</a></div><div class=\"lev2 toc-item\"><a href=\"#填充缺失值\" data-toc-modified-id=\"填充缺失值-55\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>填充缺失值</a></div><div class=\"lev1 toc-item\"><a href=\"#train\" data-toc-modified-id=\"train-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>train</a></div><div class=\"lev2 toc-item\"><a href=\"#feature-selection\" data-toc-modified-id=\"feature-selection-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>feature selection</a></div><div class=\"lev1 toc-item\"><a href=\"#predict\" data-toc-modified-id=\"predict-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>predict</a></div><div class=\"lev2 toc-item\"><a href=\"#predict-data\" data-toc-modified-id=\"predict-data-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>predict data</a></div><div class=\"lev1 toc-item\"><a href=\"#summary\" data-toc-modified-id=\"summary-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>summary</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from  sklearn import cross_validation\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import sklearn.metrics as sm\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import datetime\n",
    "random.seed(0)\n",
    "from sklearn.cross_validation import KFold\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rules(y_val,y_pred_val):\n",
    "    df_merge=pd.DataFrame(y_pred_val)\n",
    "    df_merge[1]=y_val\n",
    "    dfMergeSort=df_merge.round(8).sort(0)\n",
    "    dfMergeSort['one']=1\n",
    "    dfMergeSortGB=dfMergeSort.groupby([0,1]).sum()\n",
    "    dfFinal=dfMergeSortGB.unstack().fillna(0).cumsum()['one']\n",
    "    dfFinal[0]=dfFinal[0]/float(dfFinal[0].values[-1])\n",
    "    dfFinal[1]=dfFinal[1]/float(dfFinal[1].values[-1])\n",
    "    dfFinal['difference']=dfFinal[1]-dfFinal[0]\n",
    "    return abs(dfFinal.difference).max()\n",
    "def getBalanceData_1(X,Y,randomN,n=1):\n",
    "    # binary classification\n",
    "    #X,Y=df.ix[:,:-1],df.ix[:,-1]\n",
    "    valCounts=pd.value_counts(Y)\n",
    "    (index_max,val_max)=valCounts.argmax(),valCounts.max()\n",
    "    (index_min,val_min)=valCounts.argmin(),valCounts.min()\n",
    "#     print (index_max,val_max),(index_min,val_min)\n",
    "#     n=val_max/val_min\n",
    "    X_min,Y_min=X[Y==index_min],Y[Y==index_min]\n",
    "    X_max,Y_max=X[Y==index_max],Y[Y==index_max]\n",
    "    vn_=val_min*n\n",
    "    if vn_>=Y_max.shape[0]:\n",
    "        vn_=Y_max.shape[0]\n",
    "    print (Y_max.shape[0])/val_min\n",
    "    Y_max_new=Y_max.sample(int(vn_),random_state=randomN)\n",
    "    X_max_new=X_max.sample(int(vn_),random_state=randomN)\n",
    "    return X_min.append(X_max_new),Y_min.append(Y_max_new)\n",
    "def getBalanceData_2(X,Y,randomN):\n",
    "    # binary classification\n",
    "    #X,Y=df.ix[:,:-1],df.ix[:,-1]\n",
    "    valCounts=pd.value_counts(Y)\n",
    "    (index_max,val_max)=valCounts.argmax(),valCounts.max()\n",
    "    (index_min,val_min)=valCounts.argmin(),valCounts.min()\n",
    "#     print (index_max,val_max),(index_min,val_min)\n",
    "    n=val_max/val_min\n",
    "    X_min,Y_min=X[Y==index_min],Y[Y==index_min]\n",
    "    X_max,Y_max=X[Y==index_max],Y[Y==index_max]\n",
    "    x_col=X_min.columns\n",
    "    X_min_new=pd.DataFrame(np.repeat(X_min.values,n,axis=0),columns=x_col)\n",
    "    Y_min_new=pd.Series(np.repeat(Y_min.values,n,axis=0))\n",
    "    print X_min_new.shape,Y_min_new.shape,X_max.shape,Y_max.shape\n",
    "    return X_max.append(X_min_new),Y_max.append(Y_min_new)\n",
    "# convert time to str\n",
    "def time5folds(a):\n",
    "    res=[]\n",
    "    ab=np.array(a)/20.0\n",
    "    for i in range(len(ab)):\n",
    "        if ab[i]<=0:\n",
    "            res.append('_'+str(int(ab[i])))\n",
    "        elif ab[i]>0:\n",
    "            res.append(str(int(ab[i])))\n",
    "        else:\n",
    "            res.append(\"Na\")\n",
    "    return res\n",
    "# bank' money to + - according to deal_type\n",
    "def dealMoneyChange(arr):\n",
    "    res=[]\n",
    "    for i in arr:\n",
    "        if i[0]==1:\n",
    "            res.append(-1*i[1])\n",
    "        elif i[0]==0:\n",
    "            res.append(i[1])\n",
    "        else:\n",
    "            res.append(np.nan)\n",
    "    return res\n",
    "\n",
    "category_col = ['sex', 'occupation', 'education', 'marriage', 'residence']\n",
    "def set_dummies(data, colname):\n",
    "    for col in colname:\n",
    "        data[col] = data[col].astype('category')\n",
    "        dummy = pd.get_dummies(data[col])\n",
    "        dummy = dummy.add_prefix('{}#'.format(col))\n",
    "        data.drop(col,\n",
    "                  axis = 1,\n",
    "                  inplace = True)\n",
    "        data = data.join(dummy)\n",
    "    return data\n",
    "\n",
    "\n",
    "# 提供无bill时间的user_id 的数据\n",
    "def numofNan(df_bill_pre,df_loan_pre):\n",
    "    billLoan_pre=pd.merge(df_bill_pre,df_loan_pre,on='user_id',how='outer')\n",
    "    bgb=billLoan_pre.groupby(['user_id']).size().reset_index()\n",
    "    b0gb=billLoan_pre[billLoan_pre['bill_time']==0].groupby(['user_id']).size().reset_index()\n",
    "    gbbill=pd.merge(bgb,b0gb,on='user_id',how='outer')\n",
    "    gbbill['ratio']=gbbill['0_y']/gbbill['0_x']\n",
    "    return gbbill\n",
    "\n",
    "gbbill=numofNan(df_bill,df_loan)\n",
    "billNoDate=gbbill[gbbill['ratio']==1]\n",
    "\n",
    "# 浏览记录的ifidf\n",
    "def tdIdf(df):\n",
    "    cv=CountVectorizer()\n",
    "    tft=TfidfVectorizer()\n",
    "    \n",
    "    all_cols=list(df.columns.values)\n",
    "    dfgb=df.groubby(all_cols[:-1])\n",
    "    n=len(dfgb)\n",
    "    resAll=[]\n",
    "\n",
    "    dfs_list=list(dfgb)\n",
    "    for i in range(n):\n",
    "        res=dfs_list[i][1][all_col[-1]].values\n",
    "        res=[str(i)+\"_\" for i in res]\n",
    "        resAll.append(\" \".join(res))\n",
    "    return resAll\n",
    "\n",
    "def getXy(billNDue,n=85):\n",
    "    return billNDue.iloc[:,range(2,n)],billNDue.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bank_list=['user_id','time','deal_type','deal_money','is_salary']\n",
    "user_list=['user_id','sex','occupation','education','marriage','residence']\n",
    "browse_list=['user_id','browse_time','behaviors','behaviors_code']\n",
    "bill_list=['user_id','bill_time','bank_id','last_bill_amount','last_repayment_amount','credit_limit','current_bill_balance','least_repayment_amount','consume_num','current_bill_amount','adjust_amount',\n",
    "           'recycle_interest','avaiable_amount','cash_limit','repayment_status']\n",
    "loan_list=['user_id','loan_time']\n",
    "overdue_list=['user_id','is_overdue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"E:\\\\data_learn\\\\rong360\\\\person_1108\\\\train\\\\\"\n",
    "df_bank=pd.read_csv(path+'bank_detail_train.txt',names=bank_list)\n",
    "df_bill=pd.read_csv(path+'bill_detail_train.txt',names=bill_list)\n",
    "df_browse=pd.read_csv(path+'browse_history_train.txt',names=browse_list)\n",
    "df_loan=pd.read_csv(path+'loan_time_train.txt',names=loan_list)\n",
    "df_overdue=pd.read_csv(path+'overdue_train.txt',names=overdue_list)\n",
    "df_user_info=pd.read_csv(path+'user_info_train.txt',names=user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_user_info=pd.read_csv(path+'user_info_train.txt',names=user_list)\n",
    "user_list=['user_id','sex','occupation','education','marriage','residence']\n",
    "path=\"E:\\\\data_learn\\\\rong360\\\\person_1108\\\\test\\\\\"\n",
    "df_bank_pre=pd.read_csv(path+'bank_detail_test.txt',names=bank_list)\n",
    "df_bill_pre=pd.read_csv(path+'bill_detail_test.txt',names=bill_list)\n",
    "df_browse_pre=pd.read_csv(path+'browse_history_test.txt',names=browse_list)\n",
    "df_loan_pre=pd.read_csv(path+'loan_time_test.txt',names=loan_list)\n",
    "# df_overdue=pd.read_csv(path+'overdue_test.txt',names=overdue_list[0])\n",
    "df_user_info_pre=pd.read_csv(path+'user_info_test.txt',names=user_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各表处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user 与overdue表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_dummies=set_dummies(df_user_info,category_col)\n",
    "userDue=pd.merge(df_overdue,user_dummies,on='user_id',how='outer')\n",
    "user_dummies_pre=set_dummies(df_user_info_pre,category_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## browse\n",
    "\n",
    "- 浏览总次数 -提高f1 0.1个点\n",
    "\n",
    "- 日期都是贷款loan之前的数据\n",
    "- groupby('user_id').sum()总数有提升 为有效特征\n",
    "- predict分布的趋势与browse不同\n",
    "- 每5天的browse 分数比按behaviors_code的特征好\n",
    "\n",
    "- code数据比比例效果好，有code时比多一个sum的和效果好\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge browse and loan\n",
    "def browseloan(df_browse,df_loan):\n",
    "    df_browse['one']=1\n",
    "    bankLoan=pd.merge(df_browse,df_loan,on='user_id',how='outer')\n",
    "    bankLoan['diff']=(bankLoan['browse_time']-bankLoan['loan_time'])/86400.0\n",
    "\n",
    "    bankLoan['diffStr']=time5folds(bankLoan['diff'].values)\n",
    "    del bankLoan['diff']\n",
    "    bankLoan['str_code']=bankLoan['diffStr'].astype(str)+\"_\"+bankLoan['behaviors_code'].astype(str)\n",
    "    # fabricate feature of date\n",
    "    bankUnstack=bankLoan.groupby(['user_id','str_code']).sum()['one'].unstack()\n",
    "    bankUnstack=bankUnstack.add_prefix(\"browse_dateCode\")\n",
    "    bankBase=bankUnstack.reset_index()\n",
    "    return bankBase\n",
    "\n",
    "def browseCode(df_browse):\n",
    "    df_browse['one']=1\n",
    "    return df_browse.groupby(['user_id','behaviors_code']).sum()['one'].unstack().reset_index()\n",
    "\n",
    "# feature\n",
    "browLoan=browseloan(df_browse,df_loan)\n",
    "browLoan.to_csv(\"feature_data\\\\browe_dateCode_sum.csv\",index=None)\n",
    "browLoan=browseloan(df_browse_pre,df_loan_pre)\n",
    "browLoan.to_csv(\"feature_data\\\\browe_dateCode_sum_pre.csv\",index=None)\n",
    "\n",
    "# feature\n",
    "df_browse['one']=1\n",
    "broLoaCode=df_browse.groupby(['user_id','behaviors_code']).sum()['one'].unstack().add_prefix(\"browse_code_sum_\")\n",
    "broLoaCode['sumCode']=broLoaCode.sum(axis=1)\n",
    "broLoaCode.to_csv(\"feature_data\\\\browse_Code_sum.csv\",index=None)\n",
    "df_browse_pre['one']=1\n",
    "broLoaCode=df_browse_pre.groupby(['user_id','behaviors_code']).sum()['one'].unstack().add_prefix(\"browse_code_sum_\")\n",
    "broLoaCode['sumCode']=broLoaCode.sum(axis=1)\n",
    "broLoaCode.to_csv(\"feature_data\\\\browse_Code_sum_pre.csv\",index=None)\n",
    "\n",
    "#feature\n",
    "broTime=df_browse.ix[:,['user_id','browse_time']].groupby(['user_id']).median().add_prefix(\"browse_time_\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### browse日期范围分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 处理日期\n",
    "brmin=broLoa.ix[:,['user_id','diff']].groupby('user_id').min()\n",
    "broLoa_pre=pd.merge(df_browse_pre,df_loan_pre,on='user_id',how='outer')\n",
    "broLoa_pre['diff']=(broLoa_pre['browse_time']-broLoa_pre['loan_time'])/86400\n",
    "del broLoa_pre['browse_time'],broLoa_pre['loan_time']\n",
    "#预测集处理日期\n",
    "brmin_pre=broLoa_pre.ix[:,['user_id','diff']].groupby('user_id').min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_1=brmin.iloc[:13899,:]\n",
    "br_2=brmin.iloc[13899:13899*2,:]\n",
    "br_3=brmin.iloc[13899*2:13899*3,:]\n",
    "br_4=brmin.iloc[13899*3:13899*4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#作图表示\n",
    "plt.subplot(221)\n",
    "plt.hist(br_1[br_1.notnull().values].values,bins=40)\n",
    "plt.subplot(222)\n",
    "plt.hist(br_1[br_1.notnull().values].values,bins=40)\n",
    "plt.subplot(223)\n",
    "plt.hist(br_1[br_1.notnull().values].values,bins=40)\n",
    "plt.subplot(224)\n",
    "plt.hist(br_1[br_1.notnull().values].values,bins=40)\n",
    "\n",
    "#hist图\n",
    "plt.hist(brmin_pre[brmin_pre.notnull().values].values,bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert bank_time to dummies variables according to the sum of 5 days\n",
    "def bankloan(df_bank,df_loan):\n",
    "    bankLoan=pd.merge(df_bank,df_loan,on='user_id',how='outer')\n",
    "    bankLoan['diff']=(bankLoan['time']-bankLoan['loan_time'])/86400.0\n",
    "\n",
    "    bankLoan['diffStr']=time5folds(bankLoan['diff'].values)\n",
    "    bankLoan['diffStr01']=bankLoan['diffStr']+\"_\"+bankLoan['deal_type'].astype(str)\n",
    "    del bankLoan['diff']\n",
    "\n",
    "    # deal money to +/-\n",
    "    rerr=bankLoan.ix[:,['deal_type','deal_money']].values\n",
    "    bankLoan['dealMoney']=dealMoneyChange(rerr)\n",
    "    bankLoan['dealMoney']=bankLoan['dealMoney'].astype(float)\n",
    "    # fabricate feature of date\n",
    "    bankUnstack=bankLoan.groupby(['user_id','diffStr']).mean()['dealMoney'].unstack()\n",
    "    bankdiffstr=bankLoan.groupby(['user_id','diffStr01']).mean()['deal_money'].unstack()\n",
    "\n",
    "    bankSum=bankUnstack.add_prefix(\"bank_\").reset_index()\n",
    "    bankSumMean=bankdiffstr.add_prefix(\"code_\").reset_index()\n",
    "    return bankSum,bankSumMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bankSum,bankSumMean=bankloan(df_bank,df_loan)\n",
    "bankSum_pre,bankSumMean_pre=bankloan(df_bank_pre,df_loan_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.merge(df_overdue,bankSumMean.reindex_axis(bs[bs[0]<50000].diffStr.values,axis=1),on='user_id',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs=bankSumMean.isnull().sum().reset_index()\n",
    "df=pd.merge(df_overdue,bankSumMean.reindex_axis(bs[bs[0]<50000].diffStr01.values,axis=1),on='user_id',how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bill\n",
    "\n",
    "- important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def billloan(df_bill,df_loan):\n",
    "#     bill_overdue=df_bill\n",
    "#     bill_overdue['pay_amount']=bill_overdue['last_repayment_amount']-bill_overdue['last_bill_amount']\n",
    "    df_bill['pay_amount']=df_bill['last_repayment_amount']-df_bill['last_bill_amount']\n",
    "#     bill_overdue=df_bill.reset_index().drop('index',axis=1)\n",
    "#     res=[1]\n",
    "    \"\"\"\n",
    "    for i in range(1,df_bill.shape[0]):\n",
    "        if df_bill.ix[i,'user_id']==df_bill.ix[i-1,'user_id'] and df_bill.ix[i,'bank_id']==df_bill.ix[i-1,'bank_id'] and df_bill.ix[i,'last_bill_amount']==df_bill.ix[i-1,'current_bill_balance'] and df_bill.ix[i-1,'pay_amount']<0 and df_bill.ix[i,'pay_amount']<0:\n",
    "            res.append(-1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    df_bill['delayedTwoDays']=res\n",
    "    \"\"\"\n",
    "\n",
    "    #--------------------------------------above to deal with bill_overdue------------------------------------------#\n",
    "    billLoan=pd.merge(df_bill,df_loan,on='user_id',how='outer')\n",
    "    billLoan['diff']=(billLoan['bill_time']-billLoan['loan_time'])/86400\n",
    "#     billLoan['last_owe']=billLoan[]\n",
    "    billLoan['diff5']=time5folds(billLoan['diff'].values)\n",
    "    \n",
    "#     billLoan_=billLoan[billLoan['diff']>=0]\n",
    "#     billLoanUnstack=billLoan.ix[:,['user_id','diff5','pay_amount','delayedTwoDays']].groupby(['user_id','diff5']).mean().ix[:,['pay_amount','delayedTwoDays']].unstack()\n",
    "#     billLoaPay=billLoanUnstack['pay_amount'].add_prefix(\"pay_\").reset_index()\n",
    "#     billLoaDel=billLoanUnstack['delayedTwoDays'].add_prefix(\"delay_\").reset_index()\n",
    "    billRepay_last=billLoan.ix[:,['user_id','diff5','last_repayment_amount','pay_amount']].groupby(['user_id','diff5']).mean().unstack()['last_repayment_amount'].add_prefix(\"rep_\").reset_index()\n",
    "#     billBalance=billLoan.ix[:,['user_id','diff5','current_bill_balance']].groupby(['user_id','diff5']).mean().unstack()['current_bill_balance'].add_prefix(\"bal_\").reset_index()\n",
    "#     return billLoaDel,billLoaPay,billRepay,\n",
    "#     billLoan_=billLoan[billLoan['diff']<0]\n",
    "#     billRepay_old=billLoan_.ix[:,['user_id','diff5','last_repayment_amount']].groupby(['user_id','diff5']).mean().unstack()['last_repayment_amount'].add_prefix(\"rep_\").reset_index()\n",
    "    billRepay_payamount=billLoan.ix[:,['user_id','diff5','last_repayment_amount','pay_amount']].groupby(['user_id','diff5']).mean().unstack()['pay_amount'].add_prefix(\"pay_\").reset_index()\n",
    "    return billRepay_last,billRepay_payamount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bill 与loan结合函数\n",
    "def getbillLoan(df_bill,df_loan):\n",
    "    billLoan=pd.merge(df_bill,df_loan,on='user_id',how='outer')\n",
    "    billLoan['timediff']=((billLoan['bill_time']-billLoan['loan_time'])/86400)\n",
    "    billLoan['diffN']=time5folds(billLoan['timediff'],30)\n",
    "    billN=billLoan.ix[:,['user_id','last_bill_amount','diffN']].groupby(['user_id','diffN']).sum()['last_bill_amount'].unstack().add_prefix(\"bill_lastBill_\").reset_index()\n",
    "\n",
    "    billNDue=pd.merge(df_overdue,billN,on='user_id',how='outer')\n",
    "    return billNDue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature\n",
    "# time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bill时间与loan时间\n",
    "\n",
    "- bill的时间分loan的前后进行拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "billLoan=pd.merge(df_bill,df_loan,on='user_id',how='outer')\n",
    "billLoan['time_diff']=(billLoan['bill_time']-billLoan['loan_time'])/86400\n",
    "billLoan_old=billLoan[billLoan['time_diff']>=0]\n",
    "billLoan_new=billLoan[billLoan['time_diff']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate training set\n",
    "df_user_info=pd.read_csv(\"E:\\\\data_learn\\\\rong360\\\\person_1108\\\\train\\\\user_info_train.txt\",names=user_list)\n",
    "def gebillDue(df_bill,df_loan,df_user_info,boo):\n",
    "    billRepay_last,billRepay_payamount=billloan(df_bill,df_loan)\n",
    "    bill_ON=pd.merge(billRepay_last,billRepay_payamount,on='user_id',how='outer')\n",
    "    if boo==1:        \n",
    "    # bill_oldDue=pd.merge(df_overdue,billRepay_last,on='user_id',how='outer')\n",
    "    # bill_newDue=pd.merge(df_overdue,billRepay_payamount,on='user_id',how='outer')\n",
    "        billDue=pd.merge(df_overdue,bill_ON,on='user_id',how='outer')\n",
    "    else:\n",
    "        billDue=bill_ON\n",
    "    user_dummies=set_dummies(df_user_info,category_col)\n",
    "    billDue=pd.merge(billDue,user_dummies,on='user_id',how='outer')\n",
    "    \n",
    "    return billDue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bill 结合\n",
    "billDue_pre=gebillDue(df_bill_pre,df_loan_pre,df_user_info_pre,boo=0)\n",
    "billDue=gebillDue(df_bill,df_loan,df_user_info,boo=1)\n",
    "#保存\n",
    "billDue_.iloc[:,1:-24].to_csv(\"E:\\\\data_learn\\\\rong360\\\\bill_train.csv\",index=None)\n",
    "billDue_pre.iloc[:,:-24].to_csv(\"E:\\\\data_learn\\\\rong360\\\\bill_predict.csv\",index=None)\n",
    "billDue_=billDue.reindex_axis(['is_overdue']+billDue_pre.columns.values.tolist(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "x,y=billDue_.iloc[:,range(2,390)],billDue_.iloc[:,0]\n",
    "x_pre=billDue_pre.iloc[:,range(1,389)]\n",
    "\n",
    "x1,y1=x.iloc[:13899,:],y.iloc[:13899]\n",
    "x2,y2=x.iloc[13899:13899*2,:],y.iloc[13899:13899*2]\n",
    "x3,y3=x.iloc[13899*2:13899*3,:],y.iloc[13899*2:13899*3]\n",
    "x4,y4=x.iloc[13899*3:13899*4,:],y.iloc[13899*3:13899*4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to test bill's useful features\n",
    "clf_xgb=xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=100,objective='binary:logistic',\\\n",
    "                          min_child_weight=2,subsample=1,colsample_bytree=1,seed=0)\n",
    "\n",
    "for k in KFold(billDue_.shape[0],4):\n",
    "#     print \"this type is k[0][0]= \",k[1][0]\n",
    "#     if k[1][0]==13899:\n",
    "            x_,y_=x.iloc[k[0],],y.iloc[k[0]]\n",
    "            x_['addition']=np.array(range(13899)*3)/13899.0\n",
    "            clf_xgb.fit(x_,y_)\n",
    "            x_p,y_p=x.iloc[k[1],:],y.iloc[k[1]]\n",
    "            x_p['addition']=np.array(range(13899))/13899.0\n",
    "            y_Trpred_proba_1=clf_xgb.predict_proba(x_p)\n",
    "            y_Tr=clf_xgb.predict(x_p)\n",
    "\n",
    "            print sm.log_loss(y_p,y_Trpred_proba_1[:,1])\n",
    "            print sm.classification_report(y_p,y_Tr)\n",
    "            print rules(y_p.values,y_Trpred_proba_1[:,1])\n",
    "#             y_pre=clf_xgb.predict_proba(x_pre)[:,1]\n",
    "#             df_res_predict=pd.DataFrame(y_pre,columns=['probability'])\n",
    "#             df_res_predict['userid']=billDue_pre['user_id'].astype(int)\n",
    "clf_xgb=xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=120,objective='binary:logistic',min_child_weight=2,subsample=1,colsample_bytree=1,seed=0)\n",
    "clf_xgb.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pre=clf_xgb.predict_proba(x_pre)[:,1]\n",
    "df_res_predict=pd.DataFrame(y_pre,columns=['probability'])\n",
    "df_res_predict['userid']=billDue_pre['user_id'].astype(int)\n",
    "#output \n",
    "df_res_predict.reindex_axis(['userid','probability'],axis=1).to_csv(\"data\\\\submissionAlltrain.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#模型融合\n",
    "df_1=pd.read_csv(\"C:\\\\Users\\\\Administrator\\\\Desktop\\\\86f945e7-a278-4282-932a-0655b345a360.csv\")\n",
    "df_1.probability=(df_res_predict.probability*0.55+df_1.probability*0.45)\n",
    "# 0.7:0.3时为：42872\n",
    "#0.6：0.4时为：43166\n",
    "#0.5：0.5时为：43080\n",
    "#0.55:0.45时为：43220\n",
    "#df_1为最好的四个时：43123\n",
    "df_1.reindex_axis(['userid','probability'],axis=1).to_csv(\"data\\\\submissionAlltrainN2.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各表合并\n",
    "- bill 表列选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "```python\n",
    "billLoaDelPay=pd.merge(billLoaDel,billLoaPay,on='user_id',how='outer')\n",
    "billLoaDelPay_pre=pd.merge(billLoaDel_pre,billLoaPay_pre,on='user_id',how='outer')\n",
    "\n",
    "bankBaseUser=pd.merge(bankBase,user_dummies,on='user_id',how='outer')\n",
    "bankBaseUser_pre=pd.merge(bankBase_pre,user_dummies_pre,on='user_id',how='outer')\n",
    "\n",
    "billbank=pd.merge(billLoaDelPay,bankBaseUser,on='user_id',how='outer')\n",
    "billbank_pre=pd.merge(billLoaDelPay_pre,bankBaseUser_pre,on='user_id',how='outer')\n",
    "\n",
    "billbankDue=pd.merge(df_overdue,billbank,on='user_id',how='outer')\n",
    "billbankDue_=billbankDue.reindex_axis(['is_overdue']+billbank_pre.columns.values.tolist(),axis=1)\n",
    "\n",
    "billbankDue.to_csv(\"data\\\\billbankDue.csv\",index=None)\n",
    "billbank_pre.to_csv(\"data\\\\billbank_pre.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "billbankDue=pd.read_csv(\"data\\\\billbankDue.csv\")\n",
    "billbank_pre=pd.read_csv(\"data\\\\billbank_pre.csv\")\n",
    "billbankDue_=billbankDue.reindex_axis(['is_overdue']+billbank_pre.columns.values.tolist(),axis=1)\n",
    "\n",
    "billbankbroDue_=pd.merge(billbankDue_,browseCodTim,on='user_id',how='outer')\n",
    "billbankbro_pre=pd.merge(billbank_pre,browseCodTim_pre,on='user_id',how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计查看逾期的数据表的bill，browse以及bank表信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "loanDue=pd.merge(df_loan,df_overdue,on='user_id',how='outer')\n",
    "billLoan=pd.merge(df_bill.ix[:,range(7)],loanDue,on='user_id',how='outer')\n",
    "\n",
    "billLoan['diff']=(billLoan['bill_time']-billLoan['loan_time'])/86400\n",
    "del billLoan['bill_time'],billLoan['loan_time']\n",
    "\n",
    "rs=billLoan[billLoan['diff']>0].sort(['user_id','diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## strategy -start\n",
    "\n",
    "- 针对bill里无时间的id进行预测\n",
    "- 利用browse表\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 提供无bill时间的user_id 的数据\n",
    "\n",
    "def numofNan(df_bill_pre,df_loan_pre):\n",
    "    billLoan_pre=pd.merge(df_bill_pre,df_loan_pre,on='user_id',how='outer')\n",
    "    bgb=billLoan_pre.groupby(['user_id']).size().reset_index()\n",
    "    b0gb=billLoan_pre[billLoan_pre['bill_time']==0].groupby(['user_id']).size().reset_index()\n",
    "    gbbill=pd.merge(bgb,b0gb,on='user_id',how='outer')\n",
    "    gbbill['ratio']=gbbill['0_y']/gbbill['0_x']\n",
    "    return gbbill\n",
    "\n",
    "gbbill=numofNan(df_bill,df_loan)\n",
    "billNoDate=gbbill[gbbill['ratio']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# browse 与loan合并\n",
    "browseLoan=browseloan(df_browse,df_loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 与是否逾期表合并\n",
    "broLoaUse=pd.merge(browseLoan,billNoDate.ix[:,['user_id','ratio']],on='user_id',how='right')\n",
    "broLoaUse.drop('ratio',axis=1,inplace=True)\n",
    "useDue=pd.merge(user_dummies,df_overdue,on='user_id',how='outer')\n",
    "broLoaUseDue=pd.merge(broLoaUse,useDue,on='user_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694192569931\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.67      0.59       135\n",
      "          1       0.63      0.49      0.55       159\n",
      "\n",
      "avg / total       0.58      0.57      0.57       294\n",
      "\n",
      "0.177917540182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# 查看结果\n",
    "\n",
    "x,y=broLoaUseDue.iloc[:,range(2,1237)],broLoaUseDue.iloc[:,1237]\n",
    "clf_xgb=xgb.XGBClassifier(max_depth=4,learning_rate=0.1,n_estimators=80,objective='binary:logistic',min_child_weight=2,subsample=1,colsample_bytree=1,seed=0)\n",
    "\n",
    "x_,y_=getBalanceData_1(x,y,67)\n",
    "\n",
    "tr_x,te_x,tr_y,te_y=cross_validation.train_test_split(x_,y_,train_size=0.75,random_state=60)\n",
    "clf_xgb.fit(tr_x,tr_y)\n",
    "\n",
    "y_Trpred_proba=clf_xgb.predict_proba(te_x)\n",
    "y_Tr=clf_xgb.predict(te_x)\n",
    "\n",
    "print sm.log_loss(te_y,y_Trpred_proba[:,1])\n",
    "print sm.classification_report(te_y,y_Tr)\n",
    "print rules(te_y.values,y_Trpred_proba[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 针对 bank表\n",
    "bankSum,bankSumMean=bankloan(df_bank,df_loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bankLoaUse=pd.merge(bankSumMean,billNoDate.ix[:,['user_id','ratio']],on='user_id',how='right')\n",
    "useDue=pd.merge(user_dummies,df_overdue,on='user_id',how='outer')\n",
    "bankLoaUseDue=pd.merge(bankLoaUse,useDue,on='user_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.669782933152\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.67      0.58       135\n",
      "          1       0.62      0.46      0.53       159\n",
      "\n",
      "avg / total       0.57      0.55      0.55       294\n",
      "\n",
      "0.1928721174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "x,y=bankLoaUseDue.iloc[:,range(2,214)],bankLoaUseDue.iloc[:,214]\n",
    "clf_xgb=xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=80,objective='binary:logistic',min_child_weight=2,subsample=1,colsample_bytree=1,seed=0)\n",
    "\n",
    "x_,y_=getBalanceData_1(x,y,67)\n",
    "tr_x,te_x,tr_y,te_y=cross_validation.train_test_split(x_,y_,train_size=0.75,random_state=60)\n",
    "clf_xgb.fit(tr_x,tr_y)\n",
    "\n",
    "y_Trpred_proba=clf_xgb.predict_proba(te_x)\n",
    "y_Tr=clf_xgb.predict(te_x)\n",
    "\n",
    "print sm.log_loss(te_y,y_Trpred_proba[:,1])\n",
    "print sm.classification_report(te_y,y_Tr)\n",
    "print rules(te_y.values,y_Trpred_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bank 与browse合并\n",
    "bankbroUse=pd.merge(broLoaUse,bankLoaUse,on='user_id',how='outer')\n",
    "bankbroDue=pd.merge(bankbroUse,useDue,on='user_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bankbroDue_=bankbroDue.reindex_axis(['is_overdue']+bankbroDue_pre.columns.values.tolist(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.690278479234\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.72      0.61       135\n",
      "          1       0.65      0.45      0.54       159\n",
      "\n",
      "avg / total       0.60      0.57      0.57       294\n",
      "\n",
      "0.199720475192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "x,y=bankbroDue_.iloc[:,range(2,1204)],bankbroDue_.iloc[:,0]\n",
    "clf_xgb=xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=80,objective='binary:logistic',min_child_weight=2,subsample=1,colsample_bytree=1,seed=0)\n",
    "x_,y_=getBalanceData_1(x,y,9)\n",
    "\n",
    "tr_x,te_x,tr_y,te_y=cross_validation.train_test_split(x_,y_,train_size=0.75,random_state=60)\n",
    "clf_xgb.fit(tr_x,tr_y)\n",
    "\n",
    "y_Trpred_proba=clf_xgb.predict_proba(te_x)\n",
    "y_Tr=clf_xgb.predict(te_x)\n",
    "\n",
    "print sm.log_loss(te_y,y_Trpred_proba[:,1])\n",
    "print sm.classification_report(te_y,y_Tr)\n",
    "print rules(te_y.values,y_Trpred_proba[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测数据集整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#无bill表的predict\n",
    "gbbill_pre=numofNan(df_bill_pre,df_loan_pre)\n",
    "billNoDate_pre=gbbill_pre[gbbill_pre['ratio']==1]\n",
    "\n",
    "#获取初始browse and bank\n",
    "browseLoan=browseloan(df_browse_pre,df_loan_pre)\n",
    "bankSum,bankSumMean=bankloan(df_bank_pre,df_loan_pre)\n",
    "#user\n",
    "user_dummies_pre=set_dummies(df_user_info_pre,category_col)\n",
    "# useDue=pd.merge(user_dummies,df_overdue,on='user_id',how='outer')\n",
    "\n",
    "#bank\n",
    "bankLoaUse_pre=pd.merge(bankSumMean,billNoDate_pre.ix[:,['user_id','ratio']],on='user_id',how='right')\n",
    "#browse处理\n",
    "broLoaUse_pre=pd.merge(browseLoan,billNoDate_pre.ix[:,['user_id','ratio']],on='user_id',how='right')\n",
    "\n",
    "#bank 与browse合并\n",
    "bankbroUse_pre=pd.merge(broLoaUse_pre,bankLoaUse_pre,on='user_id',how='outer')\n",
    "bankbroDue_pre=pd.merge(bankbroUse_pre,user_dummies_pre,on='user_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>browse_0_1.0</th>\n",
       "      <th>browse_0_10.0</th>\n",
       "      <th>browse_0_3.0</th>\n",
       "      <th>browse_0_4.0</th>\n",
       "      <th>browse_0_5.0</th>\n",
       "      <th>browse_0_6.0</th>\n",
       "      <th>browse_0_7.0</th>\n",
       "      <th>browse_0_8.0</th>\n",
       "      <th>browse_0_9.0</th>\n",
       "      <th>...</th>\n",
       "      <th>marriage#1</th>\n",
       "      <th>marriage#2</th>\n",
       "      <th>marriage#3</th>\n",
       "      <th>marriage#4</th>\n",
       "      <th>marriage#5</th>\n",
       "      <th>residence#0</th>\n",
       "      <th>residence#1</th>\n",
       "      <th>residence#2</th>\n",
       "      <th>residence#3</th>\n",
       "      <th>residence#4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55805.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  browse_0_1.0  browse_0_10.0  browse_0_3.0  browse_0_4.0  \\\n",
       "0  55805.0           NaN            NaN           NaN           NaN   \n",
       "\n",
       "   browse_0_5.0  browse_0_6.0  browse_0_7.0  browse_0_8.0  browse_0_9.0  \\\n",
       "0           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "      ...       marriage#1  marriage#2  marriage#3  marriage#4  marriage#5  \\\n",
       "0     ...              1.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   residence#0  residence#1  residence#2  residence#3  residence#4  \n",
       "0          0.0          0.0          0.0          0.0          1.0  \n",
       "\n",
       "[1 rows x 1203 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankbroDue_pre.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_xgb=xgb.XGBClassifier(max_depth=3,learning_rate=1,n_estimators=80,objective='binary:logistic',min_child_weight=2,subsample=0.9,colsample_bytree=1,seed=0)\n",
    "\n",
    "x,y=bankbroDue_.iloc[:,range(2,1204)],bankbroDue_.iloc[:,0]\n",
    "x_pre=bankbroDue_pre.iloc[:,range(1,1203)]\n",
    "res_predict=[]\n",
    "for i in range(50):\n",
    "    x_,y_=getBalanceData_1(x,y,i)\n",
    "    clf_xgb.fit(x_,y_,eval_metric='logloss')\n",
    "    y_Trpred_proba=clf_xgb.predict_proba(x_pre)[:,1]\n",
    "    res_predict.append(y_Trpred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_res_predict=pd.DataFrame(res_predict).T\n",
    "pre_mean=df_res_predict.mean(axis=1).values\n",
    "df_res_predict=pd.DataFrame(pre_mean,columns=['probability'])\n",
    "df_res_predict['userid']=bankbroDue_pre['user_id'].astype(int)\n",
    "#导出结果\n",
    "df_re=df_res_predict.reindex_axis(['userid','probability'],axis=1)\n",
    "# .to_csv(\"data\\\\submissionC_6.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_=pd.read_csv(\"C:\\\\Users\\\\Administrator\\\\Desktop\\\\86f945e7-a278-4282-932a-0655b345a360.csv\")\n",
    "use_=np.array(list(set(last_.userid.values)-set(df_re.userid.values)))\n",
    "f1=last_.set_index('userid').reindex(use_).reset_index()\n",
    "f=f1.append(df_re)\n",
    "f.to_csv(\"data\\\\submissionMerge.csv\",index=None)\n",
    "# 分数下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tdIdf(df):\n",
    "    cv=CountVectorizer()\n",
    "    tft=TfidfVectorizer()\n",
    "    \n",
    "    all_cols=list(df.columns.values)\n",
    "    dfgb=df.groubby(all_cols[:-1])\n",
    "    n=len(dfgb)\n",
    "    resAll=[]\n",
    "\n",
    "    dfs_list=list(dfgb)\n",
    "    for i in range(n):\n",
    "        res=dfs_list[i][1][all_col[-1]].values\n",
    "        res=[str(i)+\"_\" for i in res]\n",
    "        resAll.append(\" \".join(res))\n",
    "        \n",
    "    return resAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.389023879557 rules: 0.458263596359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.390433669072 rules: 0.432108039462\n",
      "logloss: 0.422794401421 rules: 0.418623692093\n",
      "logloss: 0.44140105129 rules: 0.423087259234\n"
     ]
    }
   ],
   "source": [
    "clf_xgb=xgb.XGBClassifier(max_depth=3,learning_rate=0.12,n_estimators=60,objective='binary:logistic',min_child_weight=2,\\\n",
    "                          subsample=0.9,colsample_bytree=0.5,seed=0,gamma=0.1,reg_lambda=5,scale_pos_weight=3)\n",
    "# clf_xgb=xgb.XGBClassifier(max_depth=6,learning_rate=0.1,n_estimators=150,scale_pos_weight=3,objective='binary:logistic',min_child_weight=2,subsample=1,colsample_bytree=1,seed=0)\n",
    "\n",
    "for k in KFold(x_tr.shape[0],4):\n",
    "    for i in range(1):   \n",
    "            x_,y_=x_tr.iloc[k[0],],y_tr.iloc[k[0]]\n",
    "            clf_xgb.fit(x_,y_)\n",
    "            x_p,y_p=x_tr.iloc[k[1],:],y_tr.iloc[k[1]]\n",
    "            y_Tr=clf_xgb.predict(x_p)\n",
    "            print \"logloss:\",sm.log_loss(y_p,clf_xgb.predict_proba(x_p)[:,1]),\"rules:\",rules(y_p.values,clf_xgb.predict_proba(x_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充缺失值\n",
    "- need long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    x_trF,y_trF=x_tr[x_tr.iloc[:,i].T.notnull()>0].iloc[:,range(i)+range(i+1,83)],x_tr[x_tr.iloc[:,i].T.notnull()>0].iloc[:,i].values\n",
    "    clf_xgb=xgb.XGBClassifier(max_depth=3,learning_rate=0.12,n_estimators=60,objective='binary:logistic',min_child_weight=2,\\\n",
    "                          subsample=0.9,colsample_bytree=0.5,seed=0,gamma=0.1,reg_lambda=5,scale_pos_weight=3)\n",
    "    x_teF=x_tr[x_tr.iloc[:,i].T.notnull()==0].iloc[:,range(i)+range(i+1,83)]\n",
    "    clf_xgb.fit(x_trF,y_trF)\n",
    "    y_pre=clf_xgb.predict_proba(x_teF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train\n",
    "\n",
    "## feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#特征重要性选择\n",
    "def feaSelect(clf_xgb,df,df_te,n):\n",
    "    df_fi=pd.DataFrame(clf_xgb.feature_importances_)\n",
    "    df_fi['col_names']=df.columns.values\n",
    "    df_fi.sort([0],ascending=False,inplace=True)\n",
    "    \n",
    "    return df.reindex_axis(df_fi.iloc[0:n]['col_names'].values,axis=1),df_te.reindex_axis(df_fi.iloc[0:n]['col_names'].values,axis=1)\n",
    "\n",
    "df_=feaSelect(clf_xgb,browLoanDue,50)\n",
    "\n",
    "# 选取重要的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeF(df,df_):\n",
    "    return pd.merge(df,df_,on='userid',how='outer')\n",
    "def mergeAll(df):\n",
    "    df_=df[0]\n",
    "    for m in range(1,len(df)):\n",
    "        df_=mergeF(df_,df[m])\n",
    "    return df_\n",
    "def getXy(bDue,n):\n",
    "    return bDue.iloc[:,range(2,n)],bDue.iloc[:,1]\n",
    "\n",
    "def mergeIn(df,df1):\n",
    "    return pd.merge(df,df1,on='user_id',how='inner')\n",
    "\n",
    "bank_list=['user_id','time','deal_type','deal_money','is_salary']\n",
    "user_list=['user_id','sex','occupation','education','marriage','residence']\n",
    "browse_list=['user_id','browse_time','behaviors','behaviors_code']\n",
    "bill_list=['user_id','bill_time','bank_id','last_bill_amount','last_repayment_amount','credit_limit','current_bill_balance','least_repayment_amount','consume_num','current_bill_amount','adjust_amount',\n",
    "           'recycle_interest','avaiable_amount','cash_limit','repayment_status']\n",
    "loan_list=['user_id','loan_time']\n",
    "overdue_list=['user_id','is_overdue']\n",
    "##############\n",
    "df_overdue=pd.read_csv('overdue_train.txt',names=overdue_list)\n",
    "df=pd.read_csv(\"ks45.csv\")\n",
    "#############\n",
    "df_=df.set_index('userid')\n",
    "\n",
    "train=df_.iloc[:13899*3,:]\n",
    "test=df_.iloc[13899*3:13899*4,:]\n",
    "# to filter feature according to the feature's information of predict set\n",
    "def featureSec(x,n=0):\n",
    "    bs=x.notnull().sum().reset_index()\n",
    "    features=bs[bs[0]>n]['index'].values\n",
    "    res_bill=[]\n",
    "    res_bank=[]\n",
    "    res_browse=[]\n",
    "    res_other=[]\n",
    "    res_mc=[]\n",
    "    for i in features:\n",
    "        if type(i)!=long and len(i)>=4 and i[:4]=='bill':\n",
    "            res_bill.append(i)    \n",
    "    # for i in features:\n",
    "        elif type(i)!=long and len(i)>=4 and i[:4]=='bank':\n",
    "            res_bank.append(i) \n",
    "    # for i in features:\n",
    "        elif type(i)!=long and len(i)>=4 and i[:4]=='brow':\n",
    "            res_browse.append(i)\n",
    "        elif type(i)!=long and i[:1]=='K':\n",
    "            res_mc.append(i)\n",
    "        else:\n",
    "            res_other.append(i)                 \n",
    "    res_bill_=list(set(res_bill)-set(['bill_dateOweMoney_Na', 'bill_dateOweMoney__-2288','bill_dateOweMoney__-2289','bill_dateLastPay_Na',\\\n",
    "                             'bill_dateLastPay__-2288', 'bill_dateLastPay__-2289','bill_delay2Days_Na', 'bill_delay2Days__-2281',\\\n",
    "                         'bill_delay2Days__-2282', 'bill_delay2Days__-2283', 'bill_delay2Days__-2284', 'bill_delay2Days__-2285',\\\n",
    "                         'bill_delay2Days__-2286', 'bill_delay2Days__-2287', 'bill_delay2Days__-2288']))\n",
    "\n",
    "    res_bank_=list(set(res_bank)-set(['bank_dateMean_Na','bank_dateMean__-6865','bank_dateMean__-6866','bank_dateMean__-6867','bank_dateMean__-6868',\\\n",
    "                           'bank_date01Mean_Na_nan','bank_date01Mean__-6865_0.0', 'bank_date01Mean__-6866_0.0','bank_date01Mean__-6867_0.0',\\\n",
    "                            'bank_date01Mean__-6867_1.0','bank_date01Mean__-6868_0.0','bank_one_Na','bank_one__-6865', 'bank_one__-6866',\\\n",
    "                           'bank_one__-6867','bank_one__-6868']))\n",
    "    \n",
    "#     return res_mc,res_bill,res_bank,res_browse,res_other\n",
    "#     return x.reindex_axis(res_other+res_bill_+res_bank_+res_browse,axis=1)\n",
    "    return x.reindex_axis(features,axis=1)\n",
    "\n",
    "def getTrain(train,test,m=1500,n=0):\n",
    "    # the num userid of Na of train and test\n",
    "    trainTNull=train.T.notnull().sum()\n",
    "    testTNull=test.T.notnull().sum()\n",
    "\n",
    "    #feature to split m\n",
    "    Lnu=test[testTNull<m]\n",
    "    Mnu=test[testTNull>m]\n",
    "\n",
    "    #null to predict\n",
    "    LnuFs=featureSec(Lnu,n)\n",
    "#     MnuFs=featureSec(Lnu,n)\n",
    "    #to choost featrue of train of LnuFS\n",
    "    trainFsNu=train.reindex_axis(LnuFs.columns,axis=1)\n",
    "    testFs=train.reindex_axis(Lnu)\n",
    "    \n",
    "    return trainFsNu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x,y=browLoanDue.iloc[:,range(2,670)],browLoanDue.iloc[:,1]\n",
    "x,y=browLoanMeDue.iloc[:,range(2,1034)],browLoanMeDue.iloc[:,1]\n",
    "# ,25)+range(670\n",
    "clf_xgb=xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=100,objective='binary:logistic',min_child_weight=2,subsample=1,colsample_bytree=1,seed=0)\n",
    "\n",
    "tr_x,tr_y=x.iloc[:-13899],y.iloc[:-13899]\n",
    "te_x,te_y=x.iloc[-13899:],y.iloc[-13899:]\n",
    "\n",
    "clf_xgb.fit(tr_x,tr_y)\n",
    "\n",
    "tr_x,te_x=feaSelect(clf_xgb,tr_x,te_x,1032)\n",
    "clf_xgb.fit(tr_x,tr_y)\n",
    "\n",
    "y_Trpred_proba_1=clf_xgb.predict_proba(te_x)\n",
    "y_Tr=clf_xgb.predict(te_x)\n",
    "\n",
    "print sm.log_loss(te_y,y_Trpred_proba_1[:,1])\n",
    "print sm.classification_report(te_y,y_Tr)\n",
    "# y_Trpred_proba_1\n",
    "print rules(te_y.values,y_Trpred_proba_1[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rainNu=getTrain(train,test)\n",
    "df_overdue.rename_axis({\"user_id\":'userid'},axis=1,inplace=True)\n",
    "\n",
    "trainNuDue=mergeF(df_overdue,trainNu.reset_index())\n",
    "trainDue=mergeF(df_overdue,train.reset_index())\n",
    "\n",
    "\n",
    "x,y=getXy(trainDue,2723)\n",
    "clf_xgb=xgb.XGBClassifier(max_depth=3,learning_rate=0.12,n_estimators=200,objective='binary:logistic',min_child_weight=2,\\\n",
    "                          subsample=0.9,colsample_bytree=0.5,seed=0,gamma=0.1,reg_lambda=5,scale_pos_weight=3)\n",
    "for k in KFold(x.shape[0],4):\n",
    "    for i in range(1):   \n",
    "            x_,y_=x.iloc[k[0],],y.iloc[k[0]]\n",
    "            clf_xgb.fit(x_,y_)\n",
    "            x_p,y_p=x.iloc[k[1],:],y.iloc[k[1]]\n",
    "            y_Tr=clf_xgb.predict(x_p)\n",
    "            print \"logloss:\",sm.log_loss(y_p,clf_xgb.predict_proba(x_p)[:,1]),\"rules:\",rules(y_p.values,clf_xgb.predict_proba(x_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n-folds validation to fillter features\n",
    "for k in KFold(x.shape[0],4):\n",
    "        x_,y_=x.iloc[k[0],],y.iloc[k[0]]\n",
    "        clf_xgb.fit(x_,y_)\n",
    "        x_p,y_p=x.iloc[k[1],:],y.iloc[k[1]]\n",
    "\n",
    "        y_Trpred_proba_1=clf_xgb.predict_proba(x_p)\n",
    "        y_Tr=clf_xgb.predict(x_p)\n",
    "#         print sm.log_loss(y_p,y_Trpred_proba_1[:,1])\n",
    "#         print sm.classification_report(y_p,y_Tr)\n",
    "        print rules(y_p.values,y_Trpred_proba_1[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#按behaviors_code为特征\n",
    "0.327823565711\n",
    "0.316904859894\n",
    "0.239341534338\n",
    "0.185771054207\n",
    "#按behaviors为特征\n",
    "0.380620924022\n",
    "0.387980339358\n",
    "0.27439968311\n",
    "0.23419158909\n",
    "#按behaviors与code的联合为特征\n",
    "0.385134948972\n",
    "0.394074479969\n",
    "0.27439968311\n",
    "0.230750782691\n",
    "# date与behaviors结合4070个特征，每30天\n",
    "0.38325489336\n",
    "0.400398292628\n",
    "0.293119919007\n",
    "0.236834810502\n",
    "#按date与浏览数量结合为特征 每5天\n",
    "0.328382317954\n",
    "0.357284620721\n",
    "0.264699074379\n",
    "0.217536769025\n",
    "#按date与浏览数量结合为特征 每10天\n",
    "0.350186502216\n",
    "0.352584927625\n",
    "0.267909056989\n",
    "0.220725102792\n",
    "#按date与浏览数量结合为特征 每15天\n",
    "0.357030984865\n",
    "0.36769549522\n",
    "0.27481273007\n",
    "0.223671125635\n",
    "#按date与浏览数量结合为特征 每20天\n",
    "0.35055960505\n",
    "0.383146173112\n",
    "0.275715129525\n",
    "0.225415581116\n",
    "#按date与浏览数量结合为特征 每25天\n",
    "\n",
    "0.36051136463\n",
    "0.373360629329\n",
    "0.273733781366\n",
    "0.220035894813\n",
    "#按date与浏览数量结合为特征 每30天\n",
    "0.359364613254\n",
    "0.369452535776\n",
    "0.27031152764\n",
    "0.222185335758"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict\n",
    "\n",
    "## predict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfmc=pd.read_csv(\"D:\\\\Program Files (x86)\\\\qqq\\\\1533782617\\\\FileRecv\\\\ks45\\\\ks45.csv\")\n",
    "dfmcDue=pd.merge(df_overdue.rename_axis({\"user_id\":\"userid\"},axis=1),dfmc,on='userid',how='outer')\n",
    "def getFea(dfmcDue):\n",
    "    return dfmcDue.T[dfmcDue.notnull().sum()>0].T    \n",
    "sdDue=getFea(dfmcDue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_tr,y_tr=sdDue.iloc[:13899*4,range(2,2545)],sdDue.iloc[:13899*4,1]\n",
    "x_pre=sdDue.iloc[13899*4:13899*5,range(2,2545)]\n",
    "clf_xgb=xgb.XGBClassifier(max_depth=6,learning_rate=0.1,n_estimators=350,objective='binary:logistic',min_child_weight=1.5,\\\n",
    "                          subsample=0.6,colsample_bytree=0.5,seed=0,colsample_bylevel= 0.5,reg_lambda=360)\n",
    "\n",
    "clf_xgb.fit(x_tr.append(sdDue.iloc[13899*2:13899*4,range(2,2545)]),y_tr.append(sdDue.iloc[13899*2:13899*4,1]))\n",
    "#predict\n",
    "y_pre=clf_xgb.predict_proba(x_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_xgb=xgb.XGBClassifier(max_depth=6,learning_rate=0.1,n_estimators=350,objective='binary:logistic',min_child_weight=1.5,\\\n",
    "                          subsample=0.6,colsample_bytree=0.5,seed=0,colsample_bylevel= 0.5,reg_lambda=360,scale_pos_weight=3)\n",
    "\n",
    "clf_xgb.fit(x_tr.append(sdDue.iloc[13899*2:13899*4,range(2,2545)]),y_tr.append(sdDue.iloc[13899*2:13899*4,1]))\n",
    "y_pre1=clf_xgb.predict_proba(x_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scale_pos_weight不同尺度下模型融合\n",
    "# bad result\n",
    "df_=sdDue.iloc[13899*4:,0].reset_index().astype(int)\n",
    "df_['probability0']=y_pre[:,1]\n",
    "df_['probability3']=y_pre1[:,1]\n",
    "df_['probability']=df_.iloc[:,[2,3]].mean(axis=1)\n",
    "df_.ix[:,['userid','probability']].to_csv(\"data\\\\last_v1.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 选择null<1000的特征\n",
    "# best result\n",
    "clf_xgb=xgb.XGBClassifier(max_depth=6,learning_rate=0.1,n_estimators=350,objective='binary:logistic',min_child_weight=1.5,\\\n",
    "                          subsample=0.6,colsample_bytree=0.5,seed=0,colsample_bylevel= 0.5,reg_lambda=360)\n",
    "x_preF=x_pre[x_pre.T.notnull().sum()<1000]\n",
    "x_preFea=getFea(x_preF)\n",
    "x_trFea=x_tr.reindex_axis(x_preFea.columns,axis=1)\n",
    "x_preFeaNull=x_pre.reindex_axis(x_preFea.columns,axis=1)\n",
    "clf_xgb.fit(x_trFea.append(x_trFea.iloc[13899*2:13899*4]),y_tr.append(sdDue.iloc[13899*2:13899*4,1]))\n",
    "#predict\n",
    "y_preFea_1=clf_xgb.predict_proba(x_preFeaNull)\n",
    "clf_xgb.fit(x_trFea.append(x_trFea.iloc[13899*2:13899*4]),y_tr.append(sdDue.iloc[13899*2:13899*4,1]))\n",
    "#predict\n",
    "y_preFea_2=clf_xgb.predict_proba(x_preFeaNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 求预测特征少于1000时的训练特征数据\n",
    "df_nu=sdDue.iloc[13899*4:,0].reset_index().astype(int)\n",
    "df_nu['probability0']=y_preFea_1[:,1]\n",
    "df_nu['probability3']=y_preFea_2[:,1]\n",
    "df_nu['probability']=df_nu.iloc[:,[2,3]].mean(axis=1)\n",
    "df_nu.ix[:,['userid','probability']].to_csv(\"data\\\\last_nuv1.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#null 的userid\n",
    "x_preNuid=pd.DataFrame(sdDue.iloc[13899*4:][x_pre.T.notnull().sum()<1000]['userid'].values,columns=['userid'])\n",
    "#null 的列\n",
    "df_Nu=pd.merge(df_nu.ix[:,['userid','probability']],x_preNuid,on='userid',how='inner')\n",
    "# 两模型结合\n",
    "x_preHavd=pd.DataFrame(list(set(sdDue.iloc[13899*4:]['userid'].values)-set(x_preNuid.userid.values)),columns=['userid'])\n",
    "df_have=pd.merge(df_.ix[:,['userid','probability']],x_preHavd,on='userid',how='inner')\n",
    "df_all=df_Nu.append(df_have).sort_values(['userid'])\n",
    "df_all.to_csv(\"data\\\\last_v2.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best 为ks所有数据，同时再加上第四部分数据，\n",
    "# xgboost参数为：\n",
    "# (max_depth=6,learning_rate=0.1,n_estimators=300,objective='binary:logistic',min_child_weight=1.5,\\\n",
    "#                           subsample=0.6,colsample_bytree=0.5,seed=0,colsample_bylevel= 0.5,reg_lambda=360）\n",
    "# 同时\n",
    "# null 是特征为特征数少于1000的用户的特征，数据所有用户数据，并加第4部分数据\n",
    "# xgboost参数\n",
    "# (max_depth=6,learning_rate=0.1,n_estimators=300,objective='binary:logistic',min_child_weight=1.5,\\\n",
    "#                           subsample=0.6,colsample_bytree=0.5,seed=0,colsample_bylevel= 0.5,reg_lambda=360）\n",
    " \n",
    "\n",
    "## analyse\n",
    "\n",
    "# analyse the different of different model\n",
    "# path=\"D:\\\\Anaconda2\\\\code\\\\rong360\\\\data\\\\\"\n",
    "# df_2=pd.read_csv(path+\"result\\\\p458_1.csv\")\n",
    "# df_1=pd.read_csv(path+\"result\\\\p458_1.csv\")\n",
    "# df_2=pd.read_csv(path+\"result\\\\p458_1.csv\")\n",
    "# df_3=pd.read_csv(path+\"last_v2.csv\")\n",
    "# df_2=pd.read_csv(path+\"last_v1.csv\")\n",
    "# df_2=pd.read_csv(path+'allData_234_v1.csv')\n",
    "\n",
    "# df_3=pd.read_csv(path+\"allData_234_2509_250_seed0.csv\")\n",
    "\n",
    "# df_=pd.merge(df_2,df_3,on='userid',how='inner')\n",
    "# dfT=df_.sort_values(['probability_x']).reset_index(drop=True)\n",
    "# df_['sdiff']=df_['probability_x']-df_['probability_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对所有数据和bill_trian合并结果将最终结果与自己最后0.55:0.45模型融合 0.43123\n",
    "- 训练数据同上，但选取后13899-13899*4之间的部分总体数据作为预测集,0.42\n",
    "- 将训练非平衡数据平横后，负例为正例的2倍，效果较使用全部数据变差\n",
    "\n",
    "- 特征的顺序也会影响预测概率的分布\n",
    "- 对各个表获取的特征命名以各表，各类为区别，以便后续进行特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.一定要备份数据代码！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in general\n",
    "\n",
    "    > - 时间序列预测时，选择合适的训练数据而不是全部\n",
    "\n",
    "    > - 调参很重要，各种参数都要用上\n",
    "\n",
    "    > - 对于特定的预测集，分类进行预测效果更好\n",
    "\n",
    "- for some details\n",
    "\n",
    "    > - 先去重复数据\n",
    "    > - 合适地填充数据\n",
    "    > - 首先尽可能建立所有特征，并对每一类特征标记\n",
    "    > - 特征选择时\n",
    "       * 去除重复特征，如相关性为1，方差为0，与标签相关性为0等（无缺失值情况）filter\n",
    "       * 存在缺失值时，慎重去除含缺失值的特征列\n",
    "       * 根据重要性选择特征（可尽量不去特征，通过调参使之更conversation）\n",
    "       * 正则化项选择特征embeded method\n",
    "       * wrapper method 一个一个减去特征观察效果recursive featrue elimination\n",
    "    > - 调参 xgboost 多尝试，cv简单查看参数，train来直接看出最佳次数，gridsearch 选最佳参数\n",
    "    > - 详细记录每一次提交数据来源，处理过程以及预测参数和线上结果\n",
    "    > - 时间序列预测时选测相应的前几天，同时选测合适的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "160px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "564px",
    "left": "0px",
    "right": "1196px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
