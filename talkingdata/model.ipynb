{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bag of apps categories\n",
    "# Bag of labels categories\n",
    "# Include phone brand and model device\n",
    "print(\"Initialize libraries\")\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\n",
    "from sklearn import ensemble\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#------------------------------------------------- Write functions ----------------------------------------\n",
    "\n",
    "def rstr(df): return df.dtypes, df.head(3) ,df.apply(lambda x: [x.unique()]), df.apply(lambda x: [len(x.unique())]),df.shape\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 700\n",
    "np.random.seed(seed)\n",
    "datadir = 'D:\\\\talkingdata\\\\data\\\\'\n",
    "\n",
    "\n",
    "\n",
    "# Data - Events data\n",
    "# Bag of apps\n",
    "print(\"# Read app events\")\n",
    "app_events = pd.read_csv(os.path.join(datadir,'app_events.csv'), dtype={'device_id' : np.str})\n",
    "app_events.head(5)\n",
    "app_events.info()\n",
    "#print(rstr(app_events))\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "app_events= app_events.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "app_events.head(5)\n",
    "\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(os.path.join(datadir,'events.csv'), dtype={'device_id': np.str})\n",
    "events.head(5)\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_events)\n",
    "events = events.dropna()\n",
    "del app_events\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "events.info()\n",
    "# 1Gb reduced to 34 Mb\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events.loc[:,\"device_id\"].value_counts(ascending=True)\n",
    "\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "events.head(5)\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "\n",
    "##################\n",
    "#   App labels\n",
    "##################\n",
    "\n",
    "app_labels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\n",
    "label_cat = pd.read_csv(os.path.join(datadir,'label_categories.csv'))\n",
    "app_labels.info()\n",
    "label_cat.info()\n",
    "label_cat=label_cat[['label_id','category']]\n",
    "\n",
    "app_labels=app_labels.merge(label_cat,on='label_id',how='left')\n",
    "app_labels.head(3)\n",
    "events.head(3)\n",
    "#app_labels = app_labels.loc[app_labels.smaller_cat != \"unknown_unknown\"]\n",
    "\n",
    "#app_labels = app_labels.groupby(\"app_id\")[\"category\"].apply(\n",
    "#    lambda x: \";\".join(set(\"app_cat:\" + str(s) for s in x)))\n",
    "app_labels = app_labels.groupby([\"app_id\",\"category\"]).agg('size').reset_index()\n",
    "app_labels = app_labels[['app_id','category']]\n",
    "\n",
    "\n",
    "# Remove \"app_id:\" from column\n",
    "print(\"## Handling events data for merging with app lables\")\n",
    "events['app_id'] = events['app_id'].map(lambda x : x.lstrip('app_id:'))\n",
    "events['app_id'] = events['app_id'].astype(str)\n",
    "app_labels['app_id'] = app_labels['app_id'].astype(str)\n",
    "app_labels.info()\n",
    "\n",
    "print(\"## Merge\")\n",
    "\n",
    "events= pd.merge(events, app_labels, on = 'app_id',how='left').astype(str)\n",
    "#events['smaller_cat'].unique()\n",
    "\n",
    "# expand to multiple rows\n",
    "print(\"#Expand to multiple rows\")\n",
    "#events= pd.concat([pd.Series(row['device_id'], row['category'].split(';'))\n",
    "#                    for _, row in events.iterrows()]).reset_index()\n",
    "#events.columns = ['app_cat', 'device_id']\n",
    "#events.head(5)\n",
    "#print(events.info())\n",
    "\n",
    "events= events.groupby([\"device_id\",\"category\"]).agg('size').reset_index()\n",
    "events= events[['device_id','category']]\n",
    "events.head(10)\n",
    "print(\"# App labels done\")\n",
    "\n",
    "f5 = events[[\"device_id\", \"category\"]]    # app_id\n",
    "# Can % total share be included as well?\n",
    "print(\"# App category part formed\")\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "\n",
    "pbd = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'),\n",
    "                  dtype={'device_id': np.str})\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "\n",
    "\n",
    "train = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n",
    "                    dtype={'device_id': np.str})\n",
    "train.drop([\"age\", \"gender\"], axis=1, inplace=True)\n",
    "\n",
    "test = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n",
    "                   dtype={'device_id': np.str})\n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# additional process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "print(\"### ----- PART 4 ----- ###\")\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "print(\"# Concat all features\")\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "\n",
    "events = None\n",
    "Df = None\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f5.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, f5), axis=0, ignore_index=True)\n",
    "\n",
    "FLS.info()\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "len(data)\n",
    "\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "sparse_matrix.shape\n",
    "sys.getsizeof(sparse_matrix)\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "print(\"# Sparse matrix done\")\n",
    "\n",
    "del FLS\n",
    "del data\n",
    "f1 = [1]\n",
    "f5 = [1]\n",
    "f2 = [1]\n",
    "f3 = [1]\n",
    "\n",
    "events = [1]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "print(\"# Split data\")\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return sparse.csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])\n",
    "def rstr(df): return df.dtypes, df.head(3) ,df.apply(lambda x: [x.unique()]), df.apply(lambda x: [len(x.unique())]),df.shape\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_sparse_csr(\"train_sp\",train_sp)\n",
    "save_sparse_csr(\"test_sp\",test_sp)\n",
    "np.save(\"train_Y\",Y)\n",
    "np.save(\"device_id\",device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize libraries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize libraries\")\n",
    "import pandas as pd`a\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#import xgboost as xgb\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\n",
    "from sklearn import ensemble\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss \n",
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return sparse.csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])\n",
    "train_sp=load_sparse_csr(\"train_sp.npz\")\n",
    "test_sp=load_sparse_csr(\"test_sp.npz\")\n",
    "device_id=np.load(\"device_id.npy\")\n",
    "Y=np.load(\"train_Y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax1_train=train_sp.getnnz(axis=1)\n",
    "ax1_test=test_sp.getnnz(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#稀疏\n",
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_3,Y_3,train_size=0.7,random_state=10)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_jobs=2,n_estimators=100)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.08939264153\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.16      0.13      0.15      1072\n",
      "          1       0.09      0.02      0.03       908\n",
      "          2       0.10      0.01      0.02       623\n",
      "          3       0.14      0.02      0.04       992\n",
      "          4       0.11      0.03      0.05      1100\n",
      "          5       0.11      0.04      0.06       842\n",
      "          6       0.15      0.27      0.20      1646\n",
      "          7       0.16      0.35      0.22      2121\n",
      "          8       0.11      0.02      0.03      1174\n",
      "          9       0.12      0.06      0.08      1510\n",
      "         10       0.14      0.30      0.19      1824\n",
      "         11       0.17      0.13      0.14      1595\n",
      "\n",
      "avg / total       0.14      0.15      0.12     15407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as sm\n",
    "y_predpro=clf.predict_proba(X_val)\n",
    "y_pre=clf.predict(X_val)\n",
    "print sm.log_loss(y_val,y_predpro)\n",
    "print sm.classification_report(y_val,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.78715062958\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.19      0.21       726\n",
      "          1       0.18      0.04      0.06       615\n",
      "          2       0.36      0.03      0.06       433\n",
      "          3       0.31      0.04      0.08       668\n",
      "          4       0.20      0.05      0.08       757\n",
      "          5       0.21      0.07      0.11       533\n",
      "          6       0.19      0.32      0.24      1122\n",
      "          7       0.18      0.41      0.25      1330\n",
      "          8       0.29      0.05      0.08       791\n",
      "          9       0.17      0.09      0.12       907\n",
      "         10       0.18      0.37      0.24      1278\n",
      "         11       0.22      0.17      0.19      1111\n",
      "\n",
      "avg / total       0.21      0.19      0.16     10271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_3,Y_3,train_size=0.8,random_state=0)\n",
    "import sklearn.metrics as sm\n",
    "y_predpro=clf.predict_proba(X_val)\n",
    "y_pre=clf.predict(X_val)\n",
    "print sm.log_loss(y_val,y_predpro)\n",
    "print sm.classification_report(y_val,y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traini=np.array(range(74645))/74645.0\n",
    "testi=np.array(range(112071))/112071.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "mde=device_id_train.astype(\"int64\").values.reshape(74645,1)\n",
    "st_deid=sparse.csr_matrix(traini.reshape(74645,1))\n",
    "testsp=sparse.csr_matrix(testi.reshape(112071,1))\n",
    "train_sp_m=hstack([train_sp,st_deid])\n",
    "test_sp_m=hstack([test_sp,testsp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from \n",
    "train_sp_3=train_sp[train_sp.getnnz(axis=1)>3]\n",
    "Y_3=pd.DataFrame(Y)[train_sp.getnnz(axis=1)>3]\n",
    "#sam_x,sam_y=stratified_sample(train_sp_3,Y_3,5000)\n",
    "train_sp_x3=train_sp[train_sp.getnnz(axis=1)<=3]\n",
    "Y_x3=pd.DataFrame(Y)[train_sp.getnnz(axis=1)<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traini_x3=pd.DataFrame(traini)[train_sp.getnnz(axis=1)<=3]\n",
    "traini_d3=pd.DataFrame(traini)[train_sp.getnnz(axis=1)>3]\n",
    "sp_traini_x3=sparse.csr_matrix(traini_x3.values)\n",
    "sp_traini_d3=sparse.csr_matrix(traini_d3.values)\n",
    "sp_train=sparse.csr_matrix(traini.reshape(74645,1))\n",
    "sp_test=sparse.csr_matrix(testi.reshape(112071,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print sam_x.shape,sam_y.shape,train_sp_3.shape,Y_3.shape\n",
    "np.random.seed(700)\n",
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_x3,Y_x3,train_size=0.8,random_state=8)\n",
    "clf_lg=LogisticRegression(penalty='l2',n_jobs=2,C=0.12,solver='lbfgs',multi_class='multinomial',\n",
    "                          random_state=700)\n",
    "                          #class_weight='balanced')\n",
    "#best:C=0.12\n",
    "clf_lg.fit(X_train,y_train)\n",
    "\n",
    "import sklearn.metrics as sm\n",
    "y_predpro=clf_lg.predict_proba(X_val)\n",
    "y_pre=clf_lg.predict(X_val)\n",
    "print sm.log_loss(y_val,y_predpro)\n",
    "print sm.classification_report(y_val,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.40447893253\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.16      0.16      0.16       743\n",
      "          1       0.25      0.00      0.01       630\n",
      "          2       0.00      0.00      0.00       425\n",
      "          3       0.00      0.00      0.00       654\n",
      "          4       0.13      0.01      0.02       760\n",
      "          5       0.00      0.00      0.00       608\n",
      "          6       0.15      0.16      0.15      1125\n",
      "          7       0.16      0.42      0.23      1340\n",
      "          8       0.20      0.00      0.01       775\n",
      "          9       0.17      0.02      0.03       981\n",
      "         10       0.14      0.30      0.19      1213\n",
      "         11       0.15      0.29      0.19      1017\n",
      "\n",
      "avg / total       0.14      0.15      0.11     10271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predpro=clf_lg.predict_proba(X_val[X_val.getnnz(axis=1)<=3])\n",
    "y_pre=clf_lg.predict(X_val[X_val.getnnz(axis=1)<=3])\n",
    "Y_x3=pd.DataFrame(y_val)[X_val.getnnz(axis=1)<=3]\n",
    "print sm.log_loss(Y_x3,y_predpro)\n",
    "print sm.classification_report(Y_x3,y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.sparse import hstack\n",
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_x3,Y_x3,train_size=0.99,random_state=7)\n",
    "params = {}\n",
    "params['booster'] = 'gblinear'\n",
    "params['objective'] = \"multi:softprob\"\n",
    "params['eval_metric'] = 'merror'\n",
    "params['eta'] = 0.01\n",
    "params['num_class'] = 12\n",
    "params['lambda'] = 13\n",
    "params['alpha'] = 0\n",
    "params['max_depth']=4\n",
    "d_train=xgb.DMatrix(X_train,label=y_train)\n",
    "d_valid=xgb.DMatrix(X_val,label=y_val)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "clf = xgb.train(params, d_train,100, watchlist, early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.46414522999\n"
     ]
    }
   ],
   "source": [
    "train_sp_x3=X_val[X_val.getnnz(axis=1)<=3]\n",
    "y_x3=pd.DataFrame(y_val)[X_val.getnnz(axis=1)<=3]\n",
    "X_valdm=xgb.DMatrix(train_sp_x3)\n",
    "y_predpro1=clf.predict(X_valdm)\n",
    "print sm.log_loss(y_x3.values,y_predpro1)\n",
    "#2.36592422048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8s - loss: 2.4754 - acc: 0.1167 - val_loss: 2.4616 - val_acc: 0.1288\n",
      "Epoch 2/10\n",
      "8s - loss: 2.4457 - acc: 0.1252 - val_loss: 2.4360 - val_acc: 0.1305\n",
      "Epoch 3/10\n",
      "8s - loss: 2.4348 - acc: 0.1304 - val_loss: 2.4339 - val_acc: 0.1305\n",
      "Epoch 4/10\n",
      "8s - loss: 2.4307 - acc: 0.1343 - val_loss: 2.4325 - val_acc: 0.1305\n",
      "Epoch 5/10\n",
      "8s - loss: 2.4294 - acc: 0.1326 - val_loss: 2.4312 - val_acc: 0.1304\n",
      "Epoch 6/10\n",
      "8s - loss: 2.4278 - acc: 0.1381 - val_loss: 2.4291 - val_acc: 0.1335\n",
      "Epoch 7/10\n",
      "8s - loss: 2.4220 - acc: 0.1373 - val_loss: 2.4267 - val_acc: 0.1336\n",
      "Epoch 8/10\n",
      "8s - loss: 2.4231 - acc: 0.1386 - val_loss: 2.4243 - val_acc: 0.1363\n",
      "Epoch 9/10\n",
      "8s - loss: 2.4183 - acc: 0.1428 - val_loss: 2.4220 - val_acc: 0.1370\n",
      "Epoch 10/10\n",
      "8s - loss: 2.4188 - acc: 0.1412 - val_loss: 2.4204 - val_acc: 0.1378\n",
      "logloss val 2.42037132673\n"
     ]
    }
   ],
   "source": [
    "train_sp_x3=train_sp[train_sp.getnnz(axis=1)<=3]\n",
    "Y_x3=pd.DataFrame(Y)[train_sp.getnnz(axis=1)<=3].values\n",
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_x3,Y_x3,train_size=0.8,random_state=8)\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "model=baseline_model()\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 300, True), nb_epoch=10,\n",
    "                         samples_per_epoch=20000,validation_data=(X_val.todense(), y_val), verbose=2,nb_worker=2\n",
    "                         )\n",
    "# evaluate the model\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10271/10271 [==============================] - 11s    \n",
      "2.40429836739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.17      0.14      0.15       726\n",
      "          1       0.00      0.00      0.00       615\n",
      "          2       0.00      0.00      0.00       433\n",
      "          3       0.00      0.00      0.00       668\n",
      "          4       0.00      0.00      0.00       757\n",
      "          5       0.00      0.00      0.00       533\n",
      "          6       0.15      0.03      0.05      1122\n",
      "          7       0.15      0.55      0.23      1330\n",
      "          8       0.00      0.00      0.00       791\n",
      "          9       0.00      0.00      0.00       907\n",
      "         10       0.14      0.52      0.23      1278\n",
      "         11       0.00      0.00      0.00      1111\n",
      "\n",
      "avg / total       0.07      0.15      0.07     10271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_3,Y_3,train_size=0.8,random_state=0)\n",
    "import sklearn.metrics as sm\n",
    "y_predpro=model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "y_pre=model.predict_classes(X_val.todense())\n",
    "print sm.log_loss(y_val,y_predpro)\n",
    "print sm.classification_report(y_val,y_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_sp, Y, train_size=0.999, random_state=10)\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model=baseline_model()\n",
    "\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=10,\n",
    "                         samples_per_epoch=69984,class_weight=dics,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val)))\n",
    "\n",
    "print(\"# Final prediction\")\n",
    "scores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result = pd.DataFrame(scores , columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "print(result.head(1))\n",
    "result = result.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "result.to_csv('sub_bagofapps7_keras_150_pt4_50_pt2_15epoch_prelu_softmax.csv', index=True, index_label='device_id')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23290, 21425), (23290L, 1L))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sp_3=train_sp[train_sp.getnnz(axis=1)>3]\n",
    "Y_3=pd.DataFrame(Y)[train_sp.getnnz(axis=1)>3].values\n",
    "train_sp_3.shape,Y_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.98360140366\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.50      0.39        62\n",
      "          1       0.13      0.05      0.07        62\n",
      "          2       0.33      0.02      0.04        49\n",
      "          3       0.18      0.14      0.16        63\n",
      "          4       0.24      0.25      0.25        83\n",
      "          5       0.15      0.06      0.09        67\n",
      "          6       0.39      0.48      0.43        88\n",
      "          7       0.33      0.42      0.37       153\n",
      "          8       0.17      0.02      0.04        85\n",
      "          9       0.17      0.10      0.12       122\n",
      "         10       0.26      0.34      0.30       168\n",
      "         11       0.35      0.61      0.44       163\n",
      "\n",
      "avg / total       0.26      0.30      0.26      1165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_3,Y_3,train_size=0.95,random_state=3)\n",
    "clf_lg=LogisticRegression(penalty='l2',n_jobs=2,C=0.01,solver='lbfgs',multi_class='multinomial',\n",
    "                          random_state=100)\n",
    "\n",
    "clf_lg.fit(X_train,y_train)\n",
    "import sklearn.metrics as sm\n",
    "y_predpro=clf_lg.predict_proba(X_val)\n",
    "y_pre=clf_lg.predict(X_val)\n",
    "print sm.log_loss(y_val,y_predpro)\n",
    "print sm.classification_report(y_val,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_3,Y_3,train_size=0.7,random_state=3)\n",
    "clf_rf=RandomForestClassifier()\n",
    "clf_rf.fit(X_train,y_train)\n",
    "model=SelectFromModel(clf_rf,prefit=True)\n",
    "\n",
    "X_train_new=model.transform(X_train)\n",
    "X_val_new=model.transform(X_val)\n",
    "X_train.shape,X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Feature Selection\n",
      "[0]\ttrain-mlogloss:2.36098\teval-mlogloss:2.36545\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 5 rounds.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "params = {}\n",
    "params['booster'] = 'gblinear'\n",
    "params['objective'] = \"multi:softprob\"\n",
    "params['eval_metric'] = 'mlogloss'\n",
    "params['eta'] = 0.02\n",
    "params['num_class'] = 12\n",
    "params['lambda'] = 5\n",
    "params['alpha'] = 3\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_d3,Y_d3,train_size=0.7,random_state=3)\n",
    "print(\"# Feature Selection\")\n",
    "\n",
    "d_train=xgb.DMatrix(X_train,label=y_train)\n",
    "d_valid=xgb.DMatrix(X_val,label=y_val)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "clf = xgb.train(params, d_train, 1, watchlist, early_stopping_rounds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.29669698746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.19      0.21       726\n",
      "          1       0.28      0.03      0.05       615\n",
      "          2       0.56      0.02      0.04       433\n",
      "          3       0.34      0.02      0.04       668\n",
      "          4       0.22      0.06      0.09       757\n",
      "          5       0.21      0.06      0.10       533\n",
      "          6       0.21      0.32      0.25      1122\n",
      "          7       0.19      0.41      0.26      1330\n",
      "          8       0.35      0.04      0.07       791\n",
      "          9       0.18      0.09      0.12       907\n",
      "         10       0.18      0.38      0.24      1278\n",
      "         11       0.20      0.25      0.22      1111\n",
      "\n",
      "avg / total       0.24      0.20      0.17     10271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_3,Y_3,train_size=0.8,random_state=0)\n",
    "import sklearn.metrics as sm\n",
    "y_predpro=clf_lg.predict_proba(X_val)\n",
    "y_pre=clf_lg.predict(X_val)\n",
    "print sm.log_loss(y_val,y_predpro)\n",
    "print sm.classification_report(y_val,y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23290, 21425), (23290L, 1L))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sp_d3=train_sp[train_sp.getnnz(axis=1)>3]\n",
    "Y_d3=pd.DataFrame(Y)[train_sp.getnnz(axis=1)>3].values\n",
    "train_sp_d3.shape,Y_d3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_all=load_model(\"all_datafit.h5\")\n",
    "#scores_val_all = model_all.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "#print('logloss val {}'.format(log_loss(y_val, scores_val_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traini_x3=pd.DataFrame(traini)[train_sp.getnnz(axis=1)<=3]\n",
    "traini_d3=pd.DataFrame(traini)[train_sp.getnnz(axis=1)>3]\n",
    "sp_traini_x3=sparse.csr_matrix(traini_x3.values)\n",
    "sp_traini_d3=sparse.csr_matrix(traini_d3.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(700)\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.layers import MaxoutDense\n",
    "from keras import optimizers\n",
    "X_train,X_val,y_train,y_val=train_test_split(train_sp_d3,Y_d3,train_size=0.6,random_state=10)\n",
    "def baseline_model():\n",
    "    \"\"\"model=Sequential()\n",
    "    model.add(Dense(output_dim=1000, input_dim=X_train.shape[1], init='lecun_uniform', W_regularizer=l2(0.000025))) \n",
    "    model.add(Activation('relu')) \n",
    "    model.add(Dropout(0.5))    \n",
    "    model.add(Dense(50, init='lecun_uniform', W_regularizer=l2(0.000025))) \n",
    "    model.add(Activation('relu')) \n",
    "    model.add(Dropout(0.4))       \n",
    "    model.add(Dense(12, init='lecun_uniform'))\n",
    "    model.add(Activation('softmax'))    \n",
    "    opt = optimizers.Adagrad(lr=0.0035)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=X_train.shape[1], init='lecun_uniform', W_regularizer=l2(0.000025)))\n",
    "    model.add(Activation('relu')) \n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(1000, init='lecun_uniform', W_regularizer=l2(0.000025)))\n",
    "    model.add(Activation('relu')) \n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(MaxoutDense(12, init='lecun_uniform'))\n",
    "    model.add(Activation('softmax'))    \n",
    "    opt = optimizers.Adagrad(lr=0.0035)\n",
    "   # model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "  #  model.add(Dense(12, init='normal', W_regularizer=l2(0.1),activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "model=baseline_model()\n",
    "# evaluate the model\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 32, True), nb_epoch=30,\n",
    "                         samples_per_epoch=19984,validation_data=(X_val.todense(), y_val), verbose=2)\n",
    "\n",
    "\n",
    "#scores_val = model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "#print('logloss val {}'.format(log_loss(y_val, scores_val)))\n",
    "\n",
    "\"\"\"\n",
    "Epoch 1/16\n",
    "7s - loss: 2.4024 - acc: 0.1395 - val_loss: 2.3346 - val_acc: 0.1796\n",
    "Epoch 2/16\n",
    "7s - loss: 2.2871 - acc: 0.1899 - val_loss: 2.1870 - val_acc: 0.2364\n",
    "Epoch 3/16\n",
    "7s - loss: 2.1601 - acc: 0.2403 - val_loss: 2.0821 - val_acc: 0.2770\n",
    "Epoch 4/16\n",
    "7s - loss: 2.0820 - acc: 0.2647 - val_loss: 2.0329 - val_acc: 0.2884\n",
    "Epoch 5/16\n",
    "7s - loss: 2.0291 - acc: 0.2816 - val_loss: 2.0026 - val_acc: 0.2969\n",
    "Epoch 6/16\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.sparse import hstack\n",
    "X_train,X_val,y_train,y_val=train_test_split(hstack([train_sp_d3,sp_traini_d3]),Y_d3,train_size=0.6,random_state=10)\n",
    "params = {}\n",
    "params['booster'] = 'gbtree'\n",
    "params['objective'] = \"multi:softprob\"\n",
    "params['eval_metric'] = 'mlogloss'\n",
    "params['eta'] = 0.6\n",
    "params['num_class'] = 12\n",
    "params['lambda'] = 5\n",
    "params['alpha'] = 2\n",
    "params['max_depth']=4\n",
    "params['']\n",
    "d_train=xgb.DMatrix(X_train,label=y_train)\n",
    "d_valid=xgb.DMatrix(X_val,label=y_val)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "clf = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"p99_350_better.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/233 [===========================>..] - ETA: 0s1.85624476167\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.62      0.53        16\n",
      "          1       0.20      0.08      0.11        13\n",
      "          2       0.00      0.00      0.00         5\n",
      "          3       0.36      0.21      0.27        19\n",
      "          4       0.29      0.45      0.35        22\n",
      "          5       0.00      0.00      0.00         8\n",
      "          6       0.42      0.50      0.46        16\n",
      "          7       0.26      0.28      0.27        32\n",
      "          8       0.00      0.00      0.00        16\n",
      "          9       0.17      0.16      0.16        19\n",
      "         10       0.27      0.36      0.31        33\n",
      "         11       0.40      0.53      0.46        34\n",
      "\n",
      "avg / total       0.27      0.32      0.29       233\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:30: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(hstack([train_sp_3,sp_traini_d3]),Y_3,train_size=0.99,random_state=10)\n",
    "y_predpro=model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "y_pre=model.predict_classes(X_val.todense())\n",
    "print sm.log_loss(y_val,y_predpro)\n",
    "print sm.classification_report(y_val,y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对test进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112071L,), (112071, 21425))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id.shape,test_sp.shape\n",
    "#device_id[test_sp.getnnz(axis=1)<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sp_ls3=test_sp[test_sp.getnnz(axis=1)<=3]\n",
    "test_sp_mo3=test_sp[test_sp.getnnz(axis=1)>3]\n",
    "test_sp_ls3.shape,test_sp_mo3.shape\n",
    "\n",
    "\n",
    "testi_x3=pd.DataFrame(testi)[test_sp.getnnz(axis=1)<=3]\n",
    "testi_d3=pd.DataFrame(testi)[test_sp.getnnz(axis=1)>3]\n",
    "sp_testi_x3=sparse.csr_matrix(testi_x3.values)\n",
    "sp_testi_d3=sparse.csr_matrix(testi_d3.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35172, 21426)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hstack([test_sp_mo3,sp_testi_d3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores1=clf_lg.predict_proba(hstack([test_sp_ls3,sp_testi_x3]))\n",
    "result1= pd.DataFrame(scores1 , columns=lable_group.classes_)\n",
    "result1[\"device_id\"] = device_id[test_sp.getnnz(axis=1)<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xgboost:\n",
    "X_valdm=xgb.DMatrix(hstack([test_sp_ls3,sp_testi_x3]))\n",
    "scores1=clf.predict(X_valdm)\n",
    "result1= pd.DataFrame(scores1 , columns=lable_group.classes_)\n",
    "result1[\"device_id\"] = device_id[test_sp.getnnz(axis=1)<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1.to_csv(\"d:\\\\talkingdata\\\\data\\\\xgboost_ep6_labmbda1.csv\",index=False)\n",
    "result1=pd.read_csv(\"d:\\\\talkingdata\\\\data\\\\result_merge_result_ls3_c0p12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores2= model.predict_generator(generator=batch_generatorp(merge_two.tocsr(), 800, False), val_samples=test_sp_mo3.shape[0])\n",
    "result2 = pd.DataFrame(scores2 , columns=lable_group.classes_)\n",
    "result2[\"device_id\"] = device_id[test_sp.getnnz(axis=1)>3]\n",
    "result=result1.append(result2)\n",
    "\n",
    "result.to_csv(\"d:\\\\talkingdata_result\\\\data\\\\result_merge_three_adjust_keras_lg_v3_.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best=pd.read_csv(\"d:\\\\talkingdata_result\\\\data\\\\result_merge_three_adjust_keras_lg_v3_.csv\")\n",
    "df_best_set=df_best.set_index(\"device_id\")\n",
    "keras_result2=df_best_set.drop(result1['device_id'].astype(\"int64\").values)\n",
    "\n",
    "keras_lg=keras_result2.reset_index().append(result1)\n",
    "keras_lg.to_csv(\"d:\\\\talkingdata_result\\\\data\\\\keras_xgboost_v2.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
